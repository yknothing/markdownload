name: Quality Gates

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main, develop ]
  schedule:
    # Run quality checks daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  QUALITY_THRESHOLD_COVERAGE: 22  # Based on current baseline (docs/TDD/test_architecture_assessment.md)
  QUALITY_THRESHOLD_PASS_RATE: 65  # Based on current baseline 66.97%, allowing 2% margin
  QUALITY_THRESHOLD_PERFORMANCE: 300000 # 5 minutes max test time

jobs:
  quality-assessment:
    name: Quality Assessment
    runs-on: ubuntu-latest
    outputs:
      quality-score: ${{ steps.calculate-quality.outputs.score }}
      coverage-percentage: ${{ steps.coverage-check.outputs.percentage }}
      pass-rate: ${{ steps.test-results.outputs.pass-rate }}
      performance-score: ${{ steps.performance-check.outputs.score }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci
        
      - name: Ensure CI tools availability
        run: |
          echo "Installing required CI tools for reliable pipeline execution..."
          
          # Update package manager
          sudo apt-get update -qq
          
          # Install ripgrep (rg) - more performant than grep for our use cases
          sudo apt-get install -y ripgrep bc
          
          # Verify tool installation
          echo "Verifying tool installation:"
          rg --version
          bc --version
          echo "✅ All CI tools successfully installed and verified"

      - name: Run comprehensive test suite
        run: |
          echo "Running comprehensive test suite..."
          START_TIME=$(date +%s)
          npm run test:ci || echo "Some tests failed, continuing analysis..."
          END_TIME=$(date +%s)
          EXECUTION_TIME=$((END_TIME - START_TIME))
          echo "EXECUTION_TIME=$EXECUTION_TIME" >> $GITHUB_ENV

      - name: Check for prohibited fake implementations
        run: |
          echo "Checking for prohibited fake implementations in tests..."
          
          # Define search function with fallback mechanism
          search_with_fallback() {
            local pattern="$1"
            local path="$2"
            local description="$3"
            
            if command -v rg >/dev/null 2>&1; then
              # Use ripgrep (preferred for performance)
              if rg -n -S "$pattern" "$path" 2>/dev/null; then
                echo "❌ Found $description"
                return 0
              fi
            else
              echo "⚠️ ripgrep not available, falling back to grep"
              if grep -rn -E "$pattern" "$path" 2>/dev/null; then
                echo "❌ Found $description"
                return 0
              fi
            fi
            return 1
          }
          
          # Check for backgroundFunctions mock implementations
          if search_with_fallback "backgroundFunctions\s*=\s*\{" "tests" "prohibited backgroundFunctions mock implementations"; then
            echo "FAKE_IMPL_FOUND=true" >> $GITHUB_ENV
          fi
          
          # Check for generateValidFileName mocks
          if search_with_fallback "generateValidFileName\s*=\s*jest\.fn|mockGenerateValidFileName\b" "tests" "prohibited generateValidFileName mock implementations"; then
            echo "FAKE_IMPL_FOUND=true" >> $GITHUB_ENV
          fi
          
          # Check for textReplace mocks
          if search_with_fallback "textReplace\s*=\s*jest\.fn|mockTextReplace\b" "tests" "prohibited textReplace mock implementations"; then
            echo "FAKE_IMPL_FOUND=true" >> $GITHUB_ENV
          fi

          # Check for validateUri mocks (Phase 4 - warn only initially)
          if search_with_fallback "validateUri\\s*=\\s*jest\\.fn" "tests" "validateUri mock implementations (warning only)"; then
            echo "VALIDATEURI_MOCK_FOUND=true" >> $GITHUB_ENV
            echo "⚠️ validateUri mock detected - consider using real implementation"
          fi
          # Check for global function body definitions of business functions (Phase 3)
          if search_with_fallback "global\.(textReplace|generateValidFileName|validateUri|base64EncodeUnicode)\s*=\s*function\(" "tests/mocks" "prohibited global function body definitions of business functions (should import from real modules)"; then
            echo "GLOBAL_FUNCTION_BODY_FOUND=true" >> $GITHUB_ENV
          fi

          # Check for duplicated Jest base config
          if search_with_fallback "const JEST_BASE_CONFIG" "tests/run-tests.js" "duplicated Jest base configuration (should use single source of truth)"; then
            echo "CONFIG_DUPLICATE_FOUND=true" >> $GITHUB_ENV
          fi
          
          if [ "$FAKE_IMPL_FOUND" != "true" ] && [ "$CONFIG_DUPLICATE_FOUND" != "true" ] && [ "$GLOBAL_FUNCTION_BODY_FOUND" != "true" ]; then
            echo "✅ No prohibited fake implementations found"
          fi

      - name: Analyze test results
        id: test-results
        run: |
          if [ -f "coverage/test-analysis.json" ]; then
            PASS_RATE=$(node -e "
              const analysis = require('./coverage/test-analysis.json');
              console.log(analysis.summary.passRate);
            ")
            echo "pass-rate=$PASS_RATE" >> $GITHUB_OUTPUT
            echo "Test pass rate: $PASS_RATE%"
          else
            echo "pass-rate=0" >> $GITHUB_OUTPUT
            echo "❌ Test analysis file not found"
          fi

      - name: Setup math operations
        run: |
          # Define portable math operation function for cross-platform compatibility
          cat > /tmp/math_ops.sh << 'MATH_EOF'
          #!/bin/bash
          
          # Portable floating point comparison function
          # Usage: compare_float "value1" "operator" "value2"
          # Example: compare_float "85.5" ">=" "80.0"
          compare_float() {
            local val1="$1"
            local op="$2"
            local val2="$3"
            
            if command -v bc >/dev/null 2>&1; then
              # Use bc for precise floating point arithmetic
              case "$op" in
                ">=") result=$(echo "$val1 >= $val2" | bc -l) ;;
                "<=") result=$(echo "$val1 <= $val2" | bc -l) ;;
                ">")  result=$(echo "$val1 > $val2" | bc -l) ;;
                "<")  result=$(echo "$val1 < $val2" | bc -l) ;;
                "==") result=$(echo "$val1 == $val2" | bc -l) ;;
                *) echo "Unsupported operator: $op" >&2; return 1 ;;
              esac
              [ "$result" = "1" ]
            else
              # Fallback to awk for floating point comparison
              echo "⚠️ bc not available, using awk fallback for math operations"
              awk -v v1="$val1" -v op="$op" -v v2="$val2" '
              BEGIN {
                if (op == ">=") exit (v1 >= v2 ? 0 : 1)
                if (op == "<=") exit (v1 <= v2 ? 0 : 1)
                if (op == ">")  exit (v1 > v2 ? 0 : 1)
                if (op == "<")  exit (v1 < v2 ? 0 : 1)
                if (op == "==") exit (v1 == v2 ? 0 : 1)
                exit 1
              }'
            fi
          }
          
          # Portable floating point calculation
          # Usage: calculate "expression"
          # Example: calculate "85.5 * 0.4 + 90.0 * 0.35 + 100.0 * 0.25"
          calculate() {
            local expression="$1"
            
            if command -v bc >/dev/null 2>&1; then
              echo "scale=2; $expression" | bc -l
            else
              # Fallback to awk for calculations
              echo "⚠️ bc not available, using awk fallback for calculations"
              awk "BEGIN {printf \"%.2f\", $expression}"
            fi
          }
          
          # Export functions for use in subsequent steps
          export -f compare_float calculate
          MATH_EOF
          
          chmod +x /tmp/math_ops.sh
          echo "✅ Math operations helper created and ready"

      - name: Check coverage thresholds
        id: coverage-check
        run: |
          # Source math operations
          source /tmp/math_ops.sh
          if [ -f "coverage/coverage-final.json" ]; then
            COVERAGE_PERCENTAGE=$(node -e "
              const coverage = require('./coverage/coverage-final.json');
              const files = Object.values(coverage);
              const totals = files.reduce((acc, file) => ({
                lines: acc.lines + file.l.found,
                linesCovered: acc.linesCovered + file.l.hit,
                statements: acc.statements + file.s.found,
                statementsCovered: acc.statementsCovered + Object.values(file.s).reduce((sum, count) => sum + (count > 0 ? 1 : 0), 0),
                functions: acc.functions + file.f.found,
                functionsCovered: acc.functionsCovered + Object.values(file.f).reduce((sum, count) => sum + (count > 0 ? 1 : 0), 0),
                branches: acc.branches + file.b.found,
                branchesCovered: acc.branchesCovered + Object.values(file.b).reduce((sum, branch) => sum + branch.reduce((branchSum, hit) => branchSum + (hit > 0 ? 1 : 0), 0), 0)
              }), { lines: 0, linesCovered: 0, statements: 0, statementsCovered: 0, functions: 0, functionsCovered: 0, branches: 0, branchesCovered: 0 });
              
              const linesCoverage = totals.lines > 0 ? (totals.linesCovered / totals.lines * 100) : 0;
              console.log(Math.round(linesCoverage * 100) / 100);
            ")
            
            echo "percentage=$COVERAGE_PERCENTAGE" >> $GITHUB_OUTPUT
            echo "Coverage: $COVERAGE_PERCENTAGE%"
            
            # Check if coverage meets threshold using portable math operations
            if compare_float "$COVERAGE_PERCENTAGE" ">=" "$QUALITY_THRESHOLD_COVERAGE"; then
              echo "✅ Coverage $COVERAGE_PERCENTAGE% meets threshold $QUALITY_THRESHOLD_COVERAGE%"
            else
              echo "❌ Coverage $COVERAGE_PERCENTAGE% below threshold $QUALITY_THRESHOLD_COVERAGE%"
              echo "COVERAGE_FAILED=true" >> $GITHUB_ENV
            fi
          else
            echo "percentage=0" >> $GITHUB_OUTPUT
            echo "❌ Coverage file not found"
            echo "COVERAGE_FAILED=true" >> $GITHUB_ENV
          fi

      - name: Performance analysis
        id: performance-check
        run: |
          EXECUTION_TIME=${{ env.EXECUTION_TIME }}
          echo "Test execution time: ${EXECUTION_TIME}s"
          
          if [ $EXECUTION_TIME -gt $QUALITY_THRESHOLD_PERFORMANCE ]; then
            PERFORMANCE_SCORE=50
            echo "❌ Test execution time ${EXECUTION_TIME}s exceeds threshold ${QUALITY_THRESHOLD_PERFORMANCE}s"
          elif [ $EXECUTION_TIME -gt $((QUALITY_THRESHOLD_PERFORMANCE / 2)) ]; then
            PERFORMANCE_SCORE=75
            echo "⚠️ Test execution time ${EXECUTION_TIME}s is concerning"
          else
            PERFORMANCE_SCORE=100
            echo "✅ Test execution time ${EXECUTION_TIME}s is acceptable"
          fi
          
          echo "score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT

      - name: Calculate overall quality score
        id: calculate-quality
        run: |
          # Source math operations for portable calculations
          source /tmp/math_ops.sh
          
          PASS_RATE=${{ steps.test-results.outputs.pass-rate }}
          COVERAGE_PERCENTAGE=${{ steps.coverage-check.outputs.percentage }}
          PERFORMANCE_SCORE=${{ steps.performance-check.outputs.score }}
          
          # Weighted quality score calculation
          # Pass rate: 40% weight, Coverage: 35% weight, Performance: 25% weight
          QUALITY_SCORE=$(calculate "($PASS_RATE * 0.4) + ($COVERAGE_PERCENTAGE * 0.35) + ($PERFORMANCE_SCORE * 0.25)")
          
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          echo "Overall quality score: $QUALITY_SCORE"
          
          # Determine quality grade using portable comparisons
          if compare_float "$QUALITY_SCORE" ">=" "90"; then
            echo "🌟 EXCELLENT quality (A+)"
            echo "QUALITY_GRADE=A+" >> $GITHUB_ENV
          elif compare_float "$QUALITY_SCORE" ">=" "80"; then
            echo "✅ GOOD quality (A)"
            echo "QUALITY_GRADE=A" >> $GITHUB_ENV
          elif compare_float "$QUALITY_SCORE" ">=" "70"; then
            echo "⚠️ ACCEPTABLE quality (B)"
            echo "QUALITY_GRADE=B" >> $GITHUB_ENV
          elif compare_float "$QUALITY_SCORE" ">=" "60"; then
            echo "🔶 NEEDS IMPROVEMENT (C)"
            echo "QUALITY_GRADE=C" >> $GITHUB_ENV
          else
            echo "❌ CRITICAL issues (F)"
            echo "QUALITY_GRADE=F" >> $GITHUB_ENV
          fi

      - name: Generate quality report
        run: |
          cat > quality-report.md << EOF
          # Quality Assessment Report
          
          **Generated**: $(date -u)
          **Branch**: ${{ github.ref_name }}
          **Commit**: ${{ github.sha }}
          
          ## Overall Quality Score: ${{ steps.calculate-quality.outputs.score }}/100 (Grade: ${{ env.QUALITY_GRADE }})
          
          ### Metrics Breakdown
          - **Test Pass Rate**: ${{ steps.test-results.outputs.pass-rate }}% (Target: ≥${{ env.QUALITY_THRESHOLD_PASS_RATE }}%)
          - **Code Coverage**: ${{ steps.coverage-check.outputs.percentage }}% (Target: ≥${{ env.QUALITY_THRESHOLD_COVERAGE }}%)
          - **Performance Score**: ${{ steps.performance-check.outputs.score }}/100
          - **Execution Time**: ${{ env.EXECUTION_TIME }}s
          
          ### Quality Gates Status
          $([ "${{ env.COVERAGE_FAILED }}" = "true" ] && echo "❌ Coverage gate: FAILED" || echo "✅ Coverage gate: PASSED")
          $([ "${{ steps.test-results.outputs.pass-rate }}" -lt "${{ env.QUALITY_THRESHOLD_PASS_RATE }}" ] && echo "❌ Pass rate gate: FAILED" || echo "✅ Pass rate gate: PASSED")
          $([ "${{ env.EXECUTION_TIME }}" -gt "${{ env.QUALITY_THRESHOLD_PERFORMANCE }}" ] && echo "❌ Performance gate: FAILED" || echo "✅ Performance gate: PASSED")
          $([ "${{ env.FAKE_IMPL_FOUND }}" = "true" ] && echo "❌ Architecture compliance: FAILED (fake implementations detected)" || echo "✅ Architecture compliance: PASSED")
          $([ "${{ env.GLOBAL_FUNCTION_BODY_FOUND }}" = "true" ] && echo "❌ Global function body compliance: FAILED (global function definitions detected)" || echo "✅ Global function body compliance: PASSED")
          $([ "${{ env.CONFIG_DUPLICATE_FOUND }}" = "true" ] && echo "❌ Configuration consistency: FAILED (duplicate config detected)" || echo "✅ Configuration consistency: PASSED")
          
          ### Recommendations
          EOF
          
          # Add specific recommendations based on scores
          source /tmp/math_ops.sh
          PASS_RATE=${{ steps.test-results.outputs.pass-rate }}
          COVERAGE=${{ steps.coverage-check.outputs.percentage }}
          
          if compare_float "$PASS_RATE" "<" "95"; then
            echo "- 🔍 Investigate and fix failing tests to improve pass rate" >> quality-report.md
          fi
          
          if compare_float "$COVERAGE" "<" "$QUALITY_THRESHOLD_COVERAGE"; then
            echo "- 📊 Add more test coverage, especially for critical paths" >> quality-report.md
          fi
          
          if [ "${{ env.EXECUTION_TIME }}" -gt 180 ]; then
            echo "- ⚡ Optimize slow tests to improve execution time" >> quality-report.md
          fi
          
          if [ "${{ env.QUALITY_GRADE }}" = "F" ] || [ "${{ env.QUALITY_GRADE }}" = "C" ]; then
            echo "- 🚨 Immediate attention required - quality below acceptable standards" >> quality-report.md
          fi
          
          if [ "${{ env.FAKE_IMPL_FOUND }}" = "true" ]; then
            echo "- 🚫 Remove prohibited fake implementations of business logic in tests" >> quality-report.md
            echo "- 📋 Follow docs/TDD/auto_test_enhance.md for Mock guidelines" >> quality-report.md
          fi

          if [ "${{ env.GLOBAL_FUNCTION_BODY_FOUND }}" = "true" ]; then
            echo "- 🚫 Remove prohibited global function body definitions of business functions" >> quality-report.md
            echo "- 📋 Import business functions from real modules as per docs/TDD/PHASE_3_PLAN.md" >> quality-report.md
          fi

          if [ "${{ env.CONFIG_DUPLICATE_FOUND }}" = "true" ]; then
            echo "- ⚙️ Consolidate duplicated Jest configuration to single source of truth" >> quality-report.md
          fi

          if [ "$VALIDATEURI_MOCK_FOUND" = "true" ]; then
            echo "- ⚠️ validateUri mock detected - consider migrating to real implementation for better integration testing" >> quality-report.md
          fi

      - name: Comment quality report on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('quality-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

      - name: Upload quality artifacts
        uses: actions/upload-artifact@v4
        with:
          name: quality-assessment-${{ github.run_number }}
          path: |
            quality-report.md
            coverage/
          retention-days: 30

      - name: Fail job if quality gates not met
        run: |
          # Source math operations for portable comparisons
          source /tmp/math_ops.sh
          
          QUALITY_SCORE=${{ steps.calculate-quality.outputs.score }}
          
          if compare_float "$QUALITY_SCORE" "<" "60"; then
            echo "❌ Quality score $QUALITY_SCORE is below minimum threshold (60)"
            exit 1
          fi
          
          if [ "${{ env.COVERAGE_FAILED }}" = "true" ]; then
            echo "❌ Coverage gate failed"
            exit 1
          fi
          
          PASS_RATE=${{ steps.test-results.outputs.pass-rate }}
          if compare_float "$PASS_RATE" "<" "$QUALITY_THRESHOLD_PASS_RATE"; then
            echo "❌ Pass rate $PASS_RATE% is below threshold $QUALITY_THRESHOLD_PASS_RATE%"
            exit 1
          fi
          
          # Architecture compliance checks (Phase 0 gates)
          if [ "${{ env.FAKE_IMPL_FOUND }}" = "true" ]; then
            echo "❌ Architecture compliance failed: prohibited fake implementations detected"
            echo "See docs/TDD/auto_test_enhance.md for guidelines"
            exit 1
          fi

          if [ "${{ env.GLOBAL_FUNCTION_BODY_FOUND }}" = "true" ]; then
            echo "❌ Global function body compliance failed: prohibited global function definitions detected"
            echo "Import business functions from real modules as per docs/TDD/PHASE_3_PLAN.md"
            exit 1
          fi

          if [ "${{ env.CONFIG_DUPLICATE_FOUND }}" = "true" ]; then
            echo "❌ Configuration consistency failed: duplicate Jest base config detected"
            echo "Consolidate to single source of truth as per docs/TDD/test_refactor_strategy.md"
            exit 1
          fi
          
          echo "✅ All quality gates passed!"